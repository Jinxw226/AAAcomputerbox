## 隐藏层
>[!bug]+ 
我们在[[线性神经网络#线性模型]]中描述了放射变换，它是带有偏置项的线性变换。
首先，回想一下softmax回归的模型架构。 该模型通过**单个仿射变换**将我们的输入直接映射到输出，然后进行softmax操作。 如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。 但是，**仿射变换中的线性是一个很强的假设**。
### 线性模型可能会出错
**线性意味着==单调==假设**，任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）
>[!example]+
>如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。 但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。
>1.  对体温高于37摄氏度的人来说，温度越高风险越大。 然而，对体温低于37摄氏度的人来说，温度越高风险就越低。
>2. 增加位置(13,17)处像素的强度是否总是增加（或降低）图像描绘狗的似然？

*与我们前面的例子相比，这里的线性很荒谬*
	
### 在网络中加入隐藏层
我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。要做到这一点，==最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出==。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。 下面，我们以图的方式描述了多层感知机![[Pasted image 20231103225012.png|581]]
输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算

### 从线性到非线性
>[!quote]-
同之前的章节一样， 我们通过矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$ 来表示$n$个样本的小批量， 其中每个样本具有$d$个输入特征。 对于具有$h$个隐藏单元的单隐藏层多层感知机， 用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出， 称为*隐藏表示*（hidden representations）。 在数学或代码中，$\mathbf{H}$也被称为*隐藏层变量*（hidden-layer variable） 或*隐藏变量*（hidden variable）。 因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$ 和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$ 以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$ 和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。 形式上，我们按如下方式计算单隐藏层多层感知机的输出 $\mathbf{O} \in \mathbb{R}^{n \times q}$：

>[!bug]
$$\begin{split}\begin{aligned} \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}. \end{aligned}\end{split}$$
>
注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。
>
我们可以证明这一等价性，即对于任意权重值， 我们只需合并隐藏层，便可产生具有参数 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ 和$\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 的等价单层模型：
>
$$\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}$$

>[!important]
>为了发挥多层架构的潜力， 我们还需要一个额外的**关键要素： 在仿射变换之后对每个隐藏单元应用非线性的*激活函数***（activation function）$\sigma$。 激活函数的输出（例如，$\sigma(\cdot)$）被称为*活性值*（activations）。 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
>
$$\begin{split}\begin{aligned} \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\ \end{aligned}\end{split}$$
>
由于$\mathbf{X}$中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数$\sigma$也以按行的方式作用于其输入， 即一次计算一个样本。 我们在[[线性神经网络#小批量样本的矢量化]]中以相同的方式使用了softmax符号来表示按行操作。 但是本节应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值， 而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。
>
为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$和$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。

### 通用近似定理

多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。 例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。 神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。

而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，**通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数**。 我们将在后面的章节中进行更细致的讨论。

## 激活函数
_激活函数_（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。

~~~tabs
---tab ReLU函数
最受欢迎的激活函数是_修正线性单元_（Rectified linear unit，_ReLU_）， 因为它实现简单，同时在各种预测任务中表现良好。 ReLU提供了一种非常简单的非线性变换。 给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：
$$\operatorname{ReLU}(x) = \max(x, 0)$$
![[Pasted image 20231105154701.png|inlR|490]]
通俗地说，ReLU函数通过将相应的活性值设为0，仅==保留正元素并丢弃所有负元素==

注意，当输入为0时导数为0，我们可以忽略这个情况，因为输入永远都不会是0

*“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”*

导数：
```python
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```
![[Pasted image 20231105154641.png|490]]

使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU==减轻了困扰以往神经网络的梯度消失问题==

注意，ReLU函数有许多变体，包括_参数化ReLU_（Parameterized ReLU，_pReLU_） 函数 `(He et al., 2015)`。 该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：

$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)$$
---tab sigmoid函数
对于一个定义域在$\mathbb{R}$中的输入， _sigmoid函数_将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为_挤压函数_（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：

$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}$$

>[!quote]+
>在最早的神经网络中，科学家们感兴趣的是对==激发==或==不激发==的生物神经元进行建模。 因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。 阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。
>
当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个==平滑的、可微的==阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中，我们将描述利用sigmoid单元来控制时序信息流的架构。

注意，当输入接近0时，sigmoid函数接近线性变换。

```python
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```
![[Pasted image 20231105160546.png]]
sigmoid函数的导数为下面的公式：

$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)$$

sigmoid函数的导数图像如下所示。 注意，当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。

导数：
```python
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```
![[Pasted image 20231105160711.png]]
---tab tanh函数
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：

$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}$$

下面我们绘制tanh函数。 注意，当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数， ==不同的是tanh函数关于坐标系原点中心对称==。
```python
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```
tanh函数的导数是：

$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x)$$

tanh函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。
```python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```
![[Pasted image 20231105161105.png]]
~~~
	总结一下，我们现在了解了如何结合非线性函数来构建具有更强表达能力的多层神经网络架构。 顺便说一句，这些知识已经让你掌握了一个类似于1990年左右深度学习从业者的工具。 在某些方面，你比在20世纪90年代工作的任何人都有优势， 因为你可以利用功能强大的开源深度学习框架，只需几行代码就可以快速构建模型， 而以前训练这些网络需要研究人员编写数千行的C或Fortran代码。

